{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69691c9",
   "metadata": {},
   "source": [
    "# Reliability & Uncertainty Analysis of LLM Reasoning\n",
    "\n",
    "This notebook computes all research metrics and generates visualizations\n",
    "from the scored experiment data.\n",
    "\n",
    "**Metrics computed:**\n",
    "1. Accuracy (per prompt, temperature, category, difficulty)\n",
    "2. Variance / instability (disagreement rate, flip rate)\n",
    "3. Calibration / overconfidence (confidence vs correctness, ECE)\n",
    "4. Parse failure rates\n",
    "5. Failure mode summary (from manual labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.1)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('..').resolve()\n",
    "SCORES_PATH = ROOT / 'results' / 'scores.csv'\n",
    "GENERATIONS_PATH = ROOT / 'results' / 'generations.jsonl'\n",
    "FIGURES_DIR = ROOT / 'results' / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Project root: {ROOT}')\n",
    "print(f'Scores file exists: {SCORES_PATH.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1d532",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SCORES_PATH)\n",
    "\n",
    "# Basic info\n",
    "print(f'Total scored records: {len(df)}')\n",
    "print(f'Unique questions:     {df.question_id.nunique()}')\n",
    "print(f'Prompts:              {df.prompt_name.unique().tolist()}')\n",
    "print(f'Temperatures:         {sorted(df.temperature.unique())}')\n",
    "print(f'Runs per condition:   {df.run_index.nunique()}')\n",
    "print()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6207c",
   "metadata": {},
   "source": [
    "## 2. Metric A — Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy by prompt × temperature\n",
    "acc = df.groupby(['prompt_name', 'temperature'])['is_correct'].agg(['mean', 'count', 'sum'])\n",
    "acc.columns = ['accuracy', 'n', 'n_correct']\n",
    "acc['accuracy_pct'] = (acc['accuracy'] * 100).round(2)\n",
    "\n",
    "# 95% CI (Wilson score interval approximation)\n",
    "z = 1.96\n",
    "p = acc['accuracy']\n",
    "n = acc['n']\n",
    "acc['ci_low'] = ((p + z**2/(2*n) - z * np.sqrt((p*(1-p) + z**2/(4*n))/n)) / (1 + z**2/n) * 100).round(2)\n",
    "acc['ci_high'] = ((p + z**2/(2*n) + z * np.sqrt((p*(1-p) + z**2/(4*n))/n)) / (1 + z**2/n) * 100).round(2)\n",
    "\n",
    "print('=== Accuracy by Prompt × Temperature ===')\n",
    "print(acc[['accuracy_pct', 'ci_low', 'ci_high', 'n']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f710c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by category\n",
    "cat_acc = df.groupby(['category', 'prompt_name', 'temperature'])['is_correct'].mean().unstack(['prompt_name', 'temperature'])\n",
    "print('\\n=== Accuracy by Category ===')\n",
    "print((cat_acc * 100).round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by difficulty\n",
    "diff_acc = df.groupby(['difficulty', 'prompt_name', 'temperature'])['is_correct'].mean().unstack(['prompt_name', 'temperature'])\n",
    "print('\\n=== Accuracy by Difficulty ===')\n",
    "print((diff_acc * 100).round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Accuracy bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plot_data = df.groupby(['prompt_name', 'temperature'])['is_correct'].mean().reset_index()\n",
    "plot_data['label'] = plot_data['prompt_name'] + '\\n(T=' + plot_data['temperature'].astype(str) + ')'\n",
    "plot_data['accuracy'] = plot_data['is_correct'] * 100\n",
    "\n",
    "colors = sns.color_palette('muted', n_colors=len(plot_data))\n",
    "bars = ax.bar(plot_data['label'], plot_data['accuracy'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, plot_data['accuracy']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title('Accuracy by Prompt Type and Temperature', fontsize=15, fontweight='bold')\n",
    "ax.set_ylim(0, 105)\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'accuracy_by_prompt_temp.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b3f70",
   "metadata": {},
   "source": [
    "## 3. Metric B — Variance / Instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each (question, prompt, temp) group: count unique answers across runs\n",
    "variance = df.groupby(['question_id', 'prompt_name', 'temperature']).agg(\n",
    "    n_unique_answers=('parsed_answer', 'nunique'),\n",
    "    n_runs=('run_index', 'count'),\n",
    "    n_correct=('is_correct', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "variance['has_disagreement'] = (variance['n_unique_answers'] > 1).astype(int)\n",
    "variance['is_flip'] = ((variance['n_correct'] > 0) & (variance['n_correct'] < variance['n_runs'])).astype(int)\n",
    "\n",
    "# Aggregate\n",
    "var_summary = variance.groupby(['prompt_name', 'temperature']).agg(\n",
    "    disagreement_rate=('has_disagreement', 'mean'),\n",
    "    flip_rate=('is_flip', 'mean'),\n",
    "    avg_unique_answers=('n_unique_answers', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "var_summary['disagreement_pct'] = (var_summary['disagreement_rate'] * 100).round(1)\n",
    "var_summary['flip_pct'] = (var_summary['flip_rate'] * 100).round(1)\n",
    "\n",
    "print('=== Variance / Instability ===')\n",
    "print(var_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4cfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Disagreement rate bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "var_summary['label'] = var_summary['prompt_name'] + '\\n(T=' + var_summary['temperature'].astype(str) + ')'\n",
    "\n",
    "# Disagreement rate\n",
    "ax = axes[0]\n",
    "bars = ax.bar(var_summary['label'], var_summary['disagreement_pct'],\n",
    "              color=sns.color_palette('Set2', len(var_summary)), edgecolor='black', linewidth=0.5)\n",
    "for bar, val in zip(bars, var_summary['disagreement_pct']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('Disagreement Rate (%)', fontsize=12)\n",
    "ax.set_title('Answer Disagreement Across Runs', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(var_summary['disagreement_pct'].max() * 1.3, 10))\n",
    "\n",
    "# Flip rate\n",
    "ax = axes[1]\n",
    "bars = ax.bar(var_summary['label'], var_summary['flip_pct'],\n",
    "              color=sns.color_palette('Set3', len(var_summary)), edgecolor='black', linewidth=0.5)\n",
    "for bar, val in zip(bars, var_summary['flip_pct']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('Flip Rate (%)', fontsize=12)\n",
    "ax.set_title('Correctness Flip Rate (Right ↔ Wrong)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(var_summary['flip_pct'].max() * 1.3, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'variance_instability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48d74b",
   "metadata": {},
   "source": [
    "## 4. Metric C — Calibration & Overconfidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a72e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to rows with valid confidence\n",
    "df_conf = df[df['confidence'].notna() & (df['confidence'] != '')].copy()\n",
    "df_conf['confidence'] = df_conf['confidence'].astype(float)\n",
    "\n",
    "print(f'Records with confidence scores: {len(df_conf)} / {len(df)}')\n",
    "\n",
    "# Mean confidence correct vs wrong\n",
    "conf_by_correct = df_conf.groupby(['prompt_name', 'temperature', 'is_correct'])['confidence'].mean().unstack('is_correct')\n",
    "conf_by_correct.columns = ['Wrong', 'Correct']\n",
    "print('\\n=== Mean Confidence by Correctness ===')\n",
    "print(conf_by_correct.round(1).to_string())\n",
    "\n",
    "# High-confidence wrong rate\n",
    "hc_wrong = df_conf[(df_conf['confidence'] >= 80) & (~df_conf['is_correct'])]\n",
    "hc_wrong_rate = len(hc_wrong) / len(df_conf) * 100\n",
    "print(f'\\nHigh-confidence wrong rate (conf>=80 & wrong): {hc_wrong_rate:.1f}%')\n",
    "\n",
    "# Low-confidence right rate\n",
    "lc_right = df_conf[(df_conf['confidence'] < 50) & (df_conf['is_correct'])]\n",
    "lc_right_rate = len(lc_right) / len(df_conf) * 100\n",
    "print(f'Low-confidence right rate (conf<50 & correct): {lc_right_rate:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920537b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Confidence vs Correctness box plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "df_conf['correctness'] = df_conf['is_correct'].map({True: 'Correct', False: 'Wrong'})\n",
    "sns.boxplot(data=df_conf, x='prompt_name', y='confidence', hue='correctness',\n",
    "            ax=ax, palette={'Correct': '#4CAF50', 'Wrong': '#F44336'})\n",
    "\n",
    "ax.set_xlabel('Prompt Type', fontsize=13)\n",
    "ax.set_ylabel('Confidence Score', fontsize=13)\n",
    "ax.set_title('Confidence Distribution: Correct vs Wrong Answers', fontsize=15, fontweight='bold')\n",
    "ax.legend(title='', fontsize=11)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'confidence_vs_correctness.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b935e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Reliability diagram (Expected Calibration Error)\n",
    "def compute_ece(confidences, correctness, n_bins=10):\n",
    "    \"\"\"Compute Expected Calibration Error.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 100, n_bins + 1)\n",
    "    bin_centers = []\n",
    "    bin_accs = []\n",
    "    bin_confs = []\n",
    "    bin_counts = []\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n",
    "        mask = (confidences >= lo) & (confidences < hi) if i < n_bins - 1 else (confidences >= lo) & (confidences <= hi)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        bin_centers.append((lo + hi) / 2)\n",
    "        bin_accs.append(correctness[mask].mean() * 100)\n",
    "        bin_confs.append(confidences[mask].mean())\n",
    "        bin_counts.append(mask.sum())\n",
    "\n",
    "    bin_centers = np.array(bin_centers)\n",
    "    bin_accs = np.array(bin_accs)\n",
    "    bin_confs = np.array(bin_confs)\n",
    "    bin_counts = np.array(bin_counts)\n",
    "\n",
    "    ece = np.sum(bin_counts / bin_counts.sum() * np.abs(bin_accs - bin_confs))\n",
    "    return ece, bin_centers, bin_accs, bin_confs, bin_counts\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "confidences = df_conf['confidence'].values\n",
    "correctness = df_conf['is_correct'].values.astype(float)\n",
    "\n",
    "ece, centers, accs, confs, counts = compute_ece(confidences, correctness)\n",
    "\n",
    "ax.bar(centers, accs, width=8, alpha=0.6, color='steelblue', edgecolor='black', label='Accuracy')\n",
    "ax.plot([0, 100], [0, 100], 'k--', linewidth=1.5, label='Perfect calibration')\n",
    "ax.set_xlabel('Confidence (%)', fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title(f'Reliability Diagram (ECE = {ece:.1f})', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xlim(-5, 105)\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'reliability_diagram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nExpected Calibration Error (ECE): {ece:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b4acc",
   "metadata": {},
   "source": [
    "## 5. UNKNOWN Analysis (Uncertainty-Aware Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNKNOWN rate for uncertainty_aware prompt\n",
    "ua = df[df['prompt_name'] == 'uncertainty_aware'].copy()\n",
    "\n",
    "if len(ua) > 0:\n",
    "    unknown_rate = ua['is_unknown'].mean() * 100\n",
    "    error_rate = (~ua['is_correct'] & ~ua['is_unknown']).mean() * 100\n",
    "    correct_rate = ua['is_correct'].mean() * 100\n",
    "\n",
    "    print('=== Uncertainty-Aware Prompt Analysis ===')\n",
    "    print(f'  Correct:     {correct_rate:.1f}%')\n",
    "    print(f'  Wrong:       {error_rate:.1f}%')\n",
    "    print(f'  UNKNOWN:     {unknown_rate:.1f}%')\n",
    "\n",
    "    # Breakdown by temperature\n",
    "    for t in sorted(ua['temperature'].unique()):\n",
    "        sub = ua[ua['temperature'] == t]\n",
    "        print(f'\\n  Temperature = {t}:')\n",
    "        print(f'    Correct:  {sub[\"is_correct\"].mean()*100:.1f}%')\n",
    "        print(f'    Wrong:    {(~sub[\"is_correct\"] & ~sub[\"is_unknown\"]).mean()*100:.1f}%')\n",
    "        print(f'    UNKNOWN:  {sub[\"is_unknown\"].mean()*100:.1f}%')\n",
    "else:\n",
    "    print('No uncertainty_aware data found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e75f8",
   "metadata": {},
   "source": [
    "## 6. Parse Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_fail = df.groupby(['prompt_name', 'temperature'])['parse_success'].agg(\n",
    "    total='count',\n",
    "    success='sum',\n",
    ").reset_index()\n",
    "parse_fail['fail_count'] = parse_fail['total'] - parse_fail['success']\n",
    "parse_fail['fail_rate_pct'] = ((parse_fail['fail_count'] / parse_fail['total']) * 100).round(2)\n",
    "\n",
    "print('=== Parse Failure Rates ===')\n",
    "print(parse_fail.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ef9ee",
   "metadata": {},
   "source": [
    "## 7. Heatmap: Category × Prompt Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "heat_data = df.groupby(['category', 'prompt_name'])['is_correct'].mean().unstack('prompt_name') * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.heatmap(heat_data, annot=True, fmt='.1f', cmap='YlOrRd_r', cbar_kws={'label': 'Accuracy (%)'},\n",
    "            linewidths=0.5, ax=ax, vmin=0, vmax=100)\n",
    "ax.set_title('Accuracy by Category × Prompt', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / 'heatmap_category_prompt.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb4fd5",
   "metadata": {},
   "source": [
    "## 8. Failure Mode Analysis\n",
    "\n",
    "Load manually-labeled failure modes (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_modes_path = ROOT / 'results' / 'failure_modes.csv'\n",
    "\n",
    "if failure_modes_path.exists():\n",
    "    fm = pd.read_csv(failure_modes_path)\n",
    "    print(f'Loaded {len(fm)} failure mode labels')\n",
    "\n",
    "    # Distribution\n",
    "    fm_counts = fm['failure_mode'].value_counts()\n",
    "    print('\\n=== Failure Mode Distribution ===')\n",
    "    print(fm_counts.to_string())\n",
    "\n",
    "    # Plot 5: Failure mode bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    fm_counts.plot.barh(ax=ax, color=sns.color_palette('coolwarm', len(fm_counts)),\n",
    "                        edgecolor='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Count', fontsize=13)\n",
    "    ax.set_title('Failure Mode Distribution', fontsize=15, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    for i, (val, name) in enumerate(zip(fm_counts.values, fm_counts.index)):\n",
    "        ax.text(val + 0.3, i, str(val), va='center', fontsize=11, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FIGURES_DIR / 'failure_modes.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No failure_modes.csv found. Skipping failure mode analysis.')\n",
    "    print('To add: create results/failure_modes.csv with columns:')\n",
    "    print('  question_id, prompt_name, temperature, run_index, failure_mode, notes, example_snippet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2ef72",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db634746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('EXPERIMENT SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Model:               {df[\"model\"].iloc[0] if \"model\" in df.columns else \"N/A\"}')\n",
    "print(f'Total records:       {len(df)}')\n",
    "print(f'Questions:           {df.question_id.nunique()}')\n",
    "print(f'Prompts:             {df.prompt_name.unique().tolist()}')\n",
    "print(f'Temperatures:        {sorted(df.temperature.unique())}')\n",
    "print(f'Runs per condition:  {df.run_index.nunique()}')\n",
    "print(f'Overall accuracy:    {df.is_correct.mean()*100:.1f}%')\n",
    "print(f'Parse success rate:  {df.parse_success.mean()*100:.1f}%')\n",
    "print(f'UNKNOWN rate:        {df.is_unknown.mean()*100:.1f}%')\n",
    "\n",
    "if len(df_conf) > 0:\n",
    "    print(f'Mean confidence:     {df_conf.confidence.mean():.1f}')\n",
    "    print(f'ECE:                 {ece:.2f}')\n",
    "    print(f'Overconfident wrong: {hc_wrong_rate:.1f}%')\n",
    "\n",
    "print('\\n=== Best/Worst Conditions ===')\n",
    "best = df.groupby(['prompt_name', 'temperature'])['is_correct'].mean().idxmax()\n",
    "worst = df.groupby(['prompt_name', 'temperature'])['is_correct'].mean().idxmin()\n",
    "print(f'Best:  {best[0]} @ T={best[1]}  ({df.groupby([\"prompt_name\",\"temperature\"])[\"is_correct\"].mean().max()*100:.1f}%)')\n",
    "print(f'Worst: {worst[0]} @ T={worst[1]} ({df.groupby([\"prompt_name\",\"temperature\"])[\"is_correct\"].mean().min()*100:.1f}%)')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
